<!doctype html><html lang=zh dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Survey
  #


  Background & Intro
  #

只有在物理验证和签名(sign off)以及测试期间，才能衡量设计在功率、性能和面积 (PPA) 方面的质量。通常需要在中间步骤中进行纠正修改，这会导致设计的多次迭代。因此，在设计的早期阶段对 PPA 的估计将减少所需的迭代次数，增加设计的可靠性，同时深入研究flow，并最终提高结果质量 (QoR)

  NP-complete
  #

EDA 工具通常面临 NP-complete 问题，机器学习 (ML) 方法可以更好更快地解决这些问题
NP问题是一类可以通过非确定性图灵机( Non-deterministic Turing Machine)在多项式时间(Polynomial time)内解决的决策问题集合。
NP问题中最困难的问题称之为NP完全问题(NP-complete)

  ML
  #

ML 已集成到 EDA 中，尤其是逻辑综合、布局、布线、测试和验证 [23]
ML 用于预测传统方法的最佳配置。其次，ML 学习模型的特征及其性能来预测看不见的设计的行为，而无需运行昂贵的综合步骤。此外，在优化 PPA 的同时，可以通过 ML 进行设计空间探索。最后，强化学习 (RL) 探索设计空间、学习策略并执行转换，以通过“人工智能辅助设计流程”获得展望未来的最佳设计。
在 EDA 中使用 ML 的一个促成因素是 EDA 工具在设计过程中生成的大量数据。
欧几里得数据

  EDA
  #


  flow
  #


  逻辑综合
  #

逻辑综合将 HDL 中的 RTL 块映射到从给定技术库中选择的门组合，同时针对不同目标优化设计。通常，这种优化涉及时序收敛、面积和功耗之间的权衡。
描述硬件设计的 RTL 模块被映射到技术库中的逻辑单元。此映射必须满足时序约束，以在所需时钟速率下运行，同时考虑面积和功耗。因此，综合是一个可以应用 ML 的复杂优化问题。例如，提供更早的 QoR 预测以避免耗时的合成步骤的多次运行"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://pxgh02.github.io/zh/docs/Digtal/flow/notebak/EDA+GNN/"><meta property="og:site_name" content="Pxmmmm"><meta property="og:title" content="eda+gnn"><meta property="og:description" content="Survey # Background & Intro # 只有在物理验证和签名(sign off)以及测试期间，才能衡量设计在功率、性能和面积 (PPA) 方面的质量。通常需要在中间步骤中进行纠正修改，这会导致设计的多次迭代。因此，在设计的早期阶段对 PPA 的估计将减少所需的迭代次数，增加设计的可靠性，同时深入研究flow，并最终提高结果质量 (QoR)
NP-complete # EDA 工具通常面临 NP-complete 问题，机器学习 (ML) 方法可以更好更快地解决这些问题
NP问题是一类可以通过非确定性图灵机( Non-deterministic Turing Machine)在多项式时间(Polynomial time)内解决的决策问题集合。
NP问题中最困难的问题称之为NP完全问题(NP-complete)
ML # ML 已集成到 EDA 中，尤其是逻辑综合、布局、布线、测试和验证 [23] ML 用于预测传统方法的最佳配置。其次，ML 学习模型的特征及其性能来预测看不见的设计的行为，而无需运行昂贵的综合步骤。此外，在优化 PPA 的同时，可以通过 ML 进行设计空间探索。最后，强化学习 (RL) 探索设计空间、学习策略并执行转换，以通过“人工智能辅助设计流程”获得展望未来的最佳设计。 在 EDA 中使用 ML 的一个促成因素是 EDA 工具在设计过程中生成的大量数据。 欧几里得数据
EDA # flow # 逻辑综合 # 逻辑综合将 HDL 中的 RTL 块映射到从给定技术库中选择的门组合，同时针对不同目标优化设计。通常，这种优化涉及时序收敛、面积和功耗之间的权衡。
描述硬件设计的 RTL 模块被映射到技术库中的逻辑单元。此映射必须满足时序约束，以在所需时钟速率下运行，同时考虑面积和功耗。因此，综合是一个可以应用 ML 的复杂优化问题。例如，提供更早的 QoR 预测以避免耗时的合成步骤的多次运行"><meta property="og:locale" content="zh"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2025-02-20T08:28:24+00:00"><title>eda+gnn | Pxmmmm</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://pxgh02.github.io/zh/docs/Digtal/flow/notebak/EDA+GNN/><link rel=stylesheet href=/book.min.6c8b9d2a1fc95075ed7da46ca81060b39add8fff6741ac51259f768929281e2c.css integrity="sha256-bIudKh/JUHXtfaRsqBBgs5rdj/9nQaxRJZ92iSkoHiw=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/zh.search.min.b08d01413d975a4c380ae1a5bfe3265f5e9c5c6655f82c0481f7d419c7faba49.js integrity="sha256-sI0BQT2XWkw4CuGlv+MmX16cXGZV+CwEgffUGcf6ukk=" crossorigin=anonymous></script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/zh/><span>Pxmmmm</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=搜索 aria-label=搜索 maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul class=book-languages><li><input type=checkbox id=languages class=toggle>
<label for=languages class="flex justify-between"><a role=button class="flex align-center"><img src=/svg/translate.svg class=book-icon alt=Languages>
Chinese</a></label><ul><li><a href=/>English</a></li></ul></li></ul><ul><li><input type=checkbox id=section-a7a8fb3cc34544172ff5103ce3adf7cc class=toggle>
<label for=section-a7a8fb3cc34544172ff5103ce3adf7cc class="flex justify-between"><a href=/zh/docs/Other/>Other</a></label><ul><li><a href=/zh/docs/Other/Algorithms/>Algorithms</a></li><li><a href=/zh/docs/Other/git/>Git</a></li><li><a href=/zh/docs/Other/Hardware/>Hardware</a></li><li><a href=/zh/docs/Other/linux/>Linux</a></li><li><a href=/zh/docs/Other/Literature/>Literature</a></li><li><a href=/zh/docs/Other/makefile/>Makefile</a></li><li><a href=/zh/docs/Other/network/>Network</a></li><li><a href=/zh/docs/Other/Program/>Program</a></li><li><a href=/zh/docs/Other/Tools/>Tools</a></li><li><a href=/zh/docs/Other/window/>Window</a></li><li><a href=/zh/docs/Other/Writing-PPT-Presentation/>Writing Ppt Presentation</a></li></ul></li><li><input type=checkbox id=section-546194909198d851b05dc5d16088b0b9 class=toggle checked>
<label for=section-546194909198d851b05dc5d16088b0b9 class="flex justify-between"><a href=/zh/docs/Digtal/>Physical Design</a></label><ul><li><input type=checkbox id=section-9bc128912acae8559e0b27b496b7779b class=toggle>
<label for=section-9bc128912acae8559e0b27b496b7779b class="flex justify-between"><a href=/zh/docs/Digtal/flow/EDA4PR/>EDA4PR</a></label></li><li><input type=checkbox id=section-50846ac4cdec99c8bd1f8b0417acad9a class=toggle>
<label for=section-50846ac4cdec99c8bd1f8b0417acad9a class="flex justify-between"><a href=/zh/docs/Digtal/flow/flow/>Flow</a></label></li><li><input type=checkbox id=section-b2241e4da59f24976e354abe344e8d4f class=toggle>
<label for=section-b2241e4da59f24976e354abe344e8d4f class="flex justify-between"><a href=/zh/docs/Digtal/Placement/placement/>Placement</a></label></li><li><input type=checkbox id=section-259735f9bf2708d778569b162c141df5 class=toggle>
<label for=section-259735f9bf2708d778569b162c141df5 class="flex justify-between"><a href=/zh/docs/Digtal/Routing/routing2/>Routing</a></label></li><li><input type=checkbox id=section-78b8f069c0c130e55062b802fff78361 class=toggle>
<label for=section-78b8f069c0c130e55062b802fff78361 class="flex justify-between"><a href=/zh/docs/Digtal/Routing/routing1/>Routing1</a></label></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>eda+gnn</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#survey>Survey</a><ul><li><a href=#background--intro>Background & Intro</a><ul><li><a href=#np-complete>NP-complete</a></li><li><a href=#ml>ML</a></li></ul></li><li><a href=#eda>EDA</a><ul><li><a href=#flow>flow</a></li><li><a href=#挑战>挑战</a></li></ul></li><li><a href=#graphs><strong>Graphs</strong></a><ul><li><a href=#处理图结构数据的方法>处理图结构数据的方法</a></li></ul></li><li><a href=#gnn-1>GNN</a><ul><li><a href=#gnns-for-the-digital-eda-flow>GNNs for the Digital EDA Flow</a></li><li><a href=#类型>类型</a></li><li><a href=#gnn-的设计流程>GNN 的设计流程</a></li><li><a href=#gnn优点><strong>GNN优点</strong></a></li><li><a href=#models-depth><strong>Models Depth</strong></a></li><li><a href=#challenges><strong>Challenges</strong></a></li></ul></li><li><a href=#gnns-in-eda-challenges><strong>GNNs in EDA Challenges</strong></a></li><li><a href=#future-work><strong>Future Work</strong></a><ul><li><a href=#exploiting-transfer-learning>Exploiting Transfer Learning</a></li><li><a href=#exploiting-feature-information><strong>Exploiting Feature Information</strong></a></li><li><a href=#enlarging-datasets><strong>Enlarging Datasets</strong></a></li></ul></li><li><a href=#开源工具>开源工具</a></li><li><a href=#参考>参考</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=survey>Survey
<a class=anchor href=#survey>#</a></h1><h2 id=background--intro>Background & Intro
<a class=anchor href=#background--intro>#</a></h2><p>只有在物理验证和签名(sign off)以及测试期间，才能衡量设计在功率、性能和面积 (PPA) 方面的质量。通常需要在中间步骤中进行纠正修改，这会导致设计的多次迭代。因此，在设计的早期阶段对 PPA 的估计将减少所需的迭代次数，增加设计的可靠性，同时深入研究flow，并最终提高结果质量 (QoR)</p><h3 id=np-complete>NP-complete
<a class=anchor href=#np-complete>#</a></h3><p>EDA 工具通常面临 <strong>NP-complete</strong> 问题，机器学习 (ML) 方法可以更好更快地解决这些问题</p><p><strong>NP问题</strong>是一类可以通过非确定性图灵机( Non-deterministic Turing Machine)在多项式时间(Polynomial time)内解决的决策问题集合。</p><p>NP问题中最困难的问题称之为NP完全问题(NP-complete)</p><h3 id=ml>ML
<a class=anchor href=#ml>#</a></h3><p><strong>ML 已集成到 EDA 中，尤其是逻辑综合、布局、布线、测试和验证 [23]</strong>
ML 用于预测传统方法的最佳配置。其次，ML 学习模型的特征及其性能来预测看不见的设计的行为，而无需运行昂贵的综合步骤。此外，在优化 PPA 的同时，可以通过 ML 进行设计空间探索。最后，<strong>强化学习 (RL)</strong> 探索设计空间、学习策略并执行转换，以通过“人工智能辅助设计流程”获得展望未来的最佳设计。
在 EDA 中使用 ML 的一个促成因素是 <strong>EDA 工具在设计过程中生成的大量数据</strong>。
<strong>欧几里得数据</strong></p><h2 id=eda>EDA
<a class=anchor href=#eda>#</a></h2><h3 id=flow>flow
<a class=anchor href=#flow>#</a></h3><h4 id=逻辑综合><strong>逻辑综合</strong>
<a class=anchor href=#%e9%80%bb%e8%be%91%e7%bb%bc%e5%90%88>#</a></h4><p><strong>逻辑综合</strong>将 HDL 中的 RTL 块映射到从<strong>给定技术库</strong>中选择的<strong>门组合</strong>，同时针对不同目标优化设计。通常，这种优化涉及<strong>时序收敛、面积和功耗之间的权衡</strong>。</p><p>描述硬件设计的 RTL 模块被映射到技术库中的逻辑单元。<strong>此映射必须满足时序约束</strong>，以在所需时钟速率下运行，<strong>同时考虑面积和功耗</strong>。因此，<strong>综合是一个可以应用 ML 的复杂优化问题</strong>。例如，<strong>提供更早的 QoR 预测以避免耗时的合成步骤的多次运行</strong></p><p>D-SAGE [55]</p><h4 id=物理综合><strong>物理综合</strong>
<a class=anchor href=#%e7%89%a9%e7%90%86%e7%bb%bc%e5%90%88>#</a></h4><p>主要步骤</p><h5 id=布局规划><strong>布局规划</strong>
<a class=anchor href=#%e5%b8%83%e5%b1%80%e8%a7%84%e5%88%92>#</a></h5><p>在<strong>芯片布局规划</strong>中，网表的主要块和较大块被放置在二维网格上，以实现<strong>最佳 PPA</strong>，同时<strong>遵守设计规则</strong>。这可以表示为<strong>马尔可夫过程</strong>，<strong>可以使用 RL 来解决</strong>。</p><p><strong>（Edge-GNN）</strong>[44]。在 [44] 中，将 GNN 合并到 RL 框架中以对过程的不同状态进行编码，预测拥塞、密度和线路长度的奖励标签，并推广到未见的网表。它<strong>计算整个网表的节点和边缘嵌入</strong>。此 RL 代理提供与人类设计师相当或更好的结果，<strong>但需要数小时而不是数月</strong>。</p><h5 id=布局><strong>布局</strong>
<a class=anchor href=#%e5%b8%83%e5%b1%80>#</a></h5><p>设计门被映射到芯片布局的确切位置。设计越大，这个过程就越复杂。放置期间的错误决定会增加芯片面积，但也会降低芯片性能，如果导线长度高于可用布线资源，甚至会使其不适合制造。通常，需要多次放置迭代，这是耗时且计算效率低下的。因此，布局被视为一个约束优化问题。正在探索 ML，尤其是 GNN，以简化此步骤</p><p>[61] 中提供<strong>预置网络和路径长度估计</strong>。为此，他们<strong>将网表转换为有向图</strong>，其中网络代表节点，边在两个方向上连接网络。<strong>单元数、扇入、扇出大小和面积用作特征节点。使用聚类和分区结果定义边缘特征</strong>。<strong>节点的真实标签是放置后作为边界框的半周线长度获得的净长度。</strong></p><p>在 [38、39] 中，<strong>GraphSAGE</strong> 被用来构建 <strong>PL-GNN</strong></p><p>[2] 中介绍了将 EDA 中的 PPA 优化任务映射到 RL 问题的概念验证框架。这个 RL 框架使用 GraphSAGE 和无监督训练来学习可以推广到看不见的网表的节点和边缘嵌入。 GCN 是一个关键组件，因为它提取 RL 代理所需的本地和全局信息。作为研究案例，分析了 2-D 放置期间的线长度优化。</p><p>​ 在 [1] 中，自主 RL 代理以归纳方式找到最佳放置参数。网表被映射为有向图，节点和边特征是手工制作的与放置相关的属性。 GraphSAGE 学习网表嵌入并有助于推广到新设计。</p><h5 id=时钟插入><strong>时钟插入</strong>
<a class=anchor href=#%e6%97%b6%e9%92%9f%e6%8f%92%e5%85%a5>#</a></h5><h5 id=布线><strong>布线</strong>
<a class=anchor href=#%e5%b8%83%e7%ba%bf>#</a></h5><p>在 [29] 中，GAT 仅使用在物理设计后获得的特定技术门级网表来预测路由拥塞值。为此，将网表构建为无向图，其中每个门都是一个节点，边定义为通过网络连接的门之间的连接。特征节点是 50 维向量，包含有关单元类型、大小、引脚数和逻辑描述的信息。为了获得节点的真实标签，拥塞图被分成网格，每个网格的拥塞值被作为放置在该网格中的单元格的标签。 [29] 中提出的称为 CongestionNet 的架构不超过遵循公式 4 的八层 GAT。节点嵌入用于预测局部拥塞值。使用 GAT 可以提高预测质量，对于超过 100 万个单元的电路，推理时间约为 19 秒。“单V100 GPU上训练为60小时”</p><p>在 [37] 中，提出了一个端到端框架，使用<strong>基于 GNN 的长短期记忆 (LSTM) 架构来预测端口路由 TNS</strong></p><p>[9] 中，提出了两个 RL 框架来<strong>优化布局（DeepPlace）<strong>和</strong>布局布线一起优化（DeepPR）</strong></p><h4 id=验证和签收>验证和签收
<a class=anchor href=#%e9%aa%8c%e8%af%81%e5%92%8c%e7%ad%be%e6%94%b6>#</a></h4><h4 id=制造><strong>制造</strong>
<a class=anchor href=#%e5%88%b6%e9%80%a0>#</a></h4><p>制造、封装和最终测试</p><h3 id=挑战>挑战
<a class=anchor href=#%e6%8c%91%e6%88%98>#</a></h3><p><strong>(1) 它依赖于硬件设计人员的专业知识来选择合适的 EDA 工具配置</strong>，</p><p><strong>(2) RTL 的设计空间探索，逻辑综合和物理综合是手动的，因此是有限且耗时的</strong>，</p><p><strong>(3) 设计中的更正将重新初始化流程</strong>，</p><p><strong>(4) 没有早期分析或结果的可预测性</strong></p><h2 id=graphs><strong>Graphs</strong>
<a class=anchor href=#graphs>#</a></h2><h3 id=处理图结构数据的方法>处理图结构数据的方法
<a class=anchor href=#%e5%a4%84%e7%90%86%e5%9b%be%e7%bb%93%e6%9e%84%e6%95%b0%e6%8d%ae%e7%9a%84%e6%96%b9%e6%b3%95>#</a></h3><h4 id=传统浅嵌入方法>传统浅嵌入方法
<a class=anchor href=#%e4%bc%a0%e7%bb%9f%e6%b5%85%e5%b5%8c%e5%85%a5%e6%96%b9%e6%b3%95>#</a></h4><p>旨在将节点信息分解为低维嵌入向量，考虑图中节点的位置和邻域的结构[17]。最著名的图嵌入技术之一是 <strong>Random Walk [36]</strong>。在这种技术中，给定图中的起点，随机选择一个相邻点。作为第二步，再次选择随机选择的点的邻居。这是以递归方式完成的。这会生成一个随机的点序列，即随机游走。 <strong>DeepWalk</strong> [45] 和 <strong>Node2vec</strong> [16] 是众所周知的基于随机游走的图嵌入方法。</p><h5 id=缺点>缺点
<a class=anchor href=#%e7%bc%ba%e7%82%b9>#</a></h5><ul><li><p>他们的编码器将重要信息从图中映射到嵌入空间，优化每个节点的唯一嵌入向量。这<strong>在大图中的计算/统计上可能是昂贵的</strong>。</p></li><li><p>它们是转导的，即它们只能为训练期间看到的节点生成嵌入。这是一个主要缺点，因为该模型<strong>无法推广到看不见的节点</strong>。</p></li><li><p>他们<strong>没有考虑</strong>在编码过程中可以提供宝贵信息的<strong>节点特征</strong>。</p></li></ul><h5 id=其他神经网络-nn缺点><strong>其他神经网络 (NN)缺点</strong>
<a class=anchor href=#%e5%85%b6%e4%bb%96%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c-nn%e7%bc%ba%e7%82%b9>#</a></h5><p>例如 CNN，<strong>如果不将其结构映射到固定大小的向量格式，就无法直接对图形数据进行操作</strong>。首先，图没有固定的局部性或滑动窗口概念来执行卷积操作。其次，图没有固定的节点顺序。作用在图上的操作应该对节点排序或排列保持不变。</p><h4 id=gnn>GNN
<a class=anchor href=#gnn>#</a></h4><p>由于考虑了特征和拓扑信息，GNN 已被证明优于其他 ML 模型</p><p>[14] 中引入了一种称为 <strong>GNN</strong></p><p><strong>直接在图上运行的神经网络框架</strong></p><p><strong>GNN 由置换不变函数和置换等变函数组成</strong>，因此也可以在<strong>节点粒度</strong>上运行。?</p><p>GNN 旨在<strong>学习每个节点的嵌入向量</strong></p><p><strong>消息传递策略</strong>已被 GNN 的新颖架构继承</p><h2 id=gnn-1>GNN
<a class=anchor href=#gnn-1>#</a></h2><p>两篇开创性论文 [25, 41] 强调了 EDA 任务和 GNN 之间的重要联系。 [41] 中的研究<strong>首次认识到 GNN 在 EDA 中的巨大潜力</strong>。他们表示，<strong>图形结构是表示布尔函数、网表和布局的最直观方式</strong>，这些是 EDA 流程的主要关注点。他们<strong>将 GNN 视为 EDA 改进 QoR 并取代使用的传统浅层方法或数学优化技术的机会</strong></p><h3 id=gnns-for-the-digital-eda-flow>GNNs for the Digital EDA Flow
<a class=anchor href=#gnns-for-the-digital-eda-flow>#</a></h3><p><img src=C:%5cUsers%5cPxmmmm.DESKTOP-G18CTQU%5cAppData%5cRoaming%5cTypora%5ctypora-user-images%5cimage-20240903203957347.png alt=image-20240903203957347></p><h3 id=类型>类型
<a class=anchor href=#%e7%b1%bb%e5%9e%8b>#</a></h3><h4 id=recurrent-graph-neural-networksrecgnns><strong>Recurrent Graph Neural Networks</strong>（<strong>RecGNNs</strong>）
<a class=anchor href=#recurrent-graph-neural-networksrecgnns>#</a></h4><p>通过假设<strong>节点与其邻居交换信息直到达到稳定点来循环处理节点信息</strong></p><h4 id=convolutional-graph-neural-networks><strong>Convolutional Graph Neural Networks</strong>
<a class=anchor href=#convolutional-graph-neural-networks>#</a></h4><p><strong>卷积图神经网络 (ConvGNN)</strong> 旨在通过堆叠多个图卷积层来学习嵌入向量。与共享权重的 RecGNN 不同，ConvGNN <strong>每层使用不同的权重</strong>。 ConvGNN 是 <strong>CNN 对图形数据的泛化</strong>，分为光谱和空间方法 [60]：</p><p>GCN属于这种</p><h4 id=graph-autoencodersgae><strong>Graph Autoencoders</strong>（GAE）
<a class=anchor href=#graph-autoencodersgae>#</a></h4><p><strong>图自动编码器 (GAE)</strong> [60] 属于<strong>无监督框架</strong>家族，因为训练数据没有真实标签。因此，<strong>计算的损失取决于整个图的拓扑信息</strong>，包括<strong>节点和边缘特征</strong>[58]。</p><p>用于两种类型的任务：<strong>基于图的表示学习和图生成</strong></p><p>[28] 中提出的最常用的 GAE 之一中，<strong>GCN 被用作编码器来提取图嵌入</strong>。</p><h4 id=spatial-temporal-graph-neural-networks-stgnn><strong>Spatial-Temporal Graph Neural Networks</strong> (STGNN)
<a class=anchor href=#spatial-temporal-graph-neural-networks-stgnn>#</a></h4><p>许多实际应用中，<strong>输入图的结构会随时间发生变化</strong>，即矩阵 A 和 X 沿时间轴变化。使用时空图的一个例子是<strong>计算机视觉领域的动作识别问题</strong>[63]。</p><h3 id=gnn-的设计流程>GNN 的设计流程
<a class=anchor href=#gnn-%e7%9a%84%e8%ae%be%e8%ae%a1%e6%b5%81%e7%a8%8b>#</a></h3><h4 id=graph-definition><strong>Graph Definition</strong>
<a class=anchor href=#graph-definition>#</a></h4><p>定义每个节点或边缘的特征向量。</p><h4 id=task-definition><strong>Task Definition</strong>
<a class=anchor href=#task-definition>#</a></h4><p>基于要解决的问题，<strong>定义任务的粒度和监督设置</strong>。 GNN 的任务分为三个级别：<strong>节点级别、边级别和图级别</strong></p><p>我们以不同的方式训练模型：监督，如果所有节点都被标记；半监督，如果只知道一些标签；和无监督的，当没有标签可用时 [60]。</p><p><strong>目标任务和监督设置共同决定了训练期间要使用的损失函数</strong>。例如，<strong>节点级监督回归任务需要均方误差 (MSE) 函数</strong>作为训练集中所有节点的损失函数。在<strong>节点级半监督分类任务的情况下，交叉熵损失</strong>可用于少数提供的标记节点。？</p><h4 id=model-definition><strong>Model Definition</strong>
<a class=anchor href=#model-definition>#</a></h4><p>在 [72] 中，GNN 模型被定义为堆<strong>叠计算模块</strong>的概要</p><p>三种不同类型的模块：<strong>传播、采样和池化</strong>。</p><p><strong>传播模块在保留特征和拓扑信息的节点之间传播和聚合信息</strong>。由残差神经网络（ResNets）[21] 激发的跳跃连接机制也被认为是一个传播模块。</p><p><strong>采样：<strong>在 GNN 中，一个节点的信息被聚合到其上一层邻居的信息中。因此，深度 GNN 会导致需要考虑和聚合的邻居呈指数增长。采样模块减轻了邻居爆炸。采样粒度可以是节点、层和子采样级别。在节点级别，通过考虑每个节点的固定数量的邻居来限制邻域大小。在层级，每层只考虑固定数量的节点进行聚合。最后，子图级采样将邻域搜索限制为采样子图[72]。
<strong>池化</strong>：<strong>池化层驱动的池化模块在减小图的大小的同时获得更一般的特征</strong>。 <strong>定向池化</strong>，也称为读出或全局池化，它对节点特征应用节点操作以获得图级表示；和</strong>分层池化</strong>，它遵循分层模式并按层学习图表示[72]。</p><h3 id=gnn优点><strong>GNN优点</strong>
<a class=anchor href=#gnn%e4%bc%98%e7%82%b9>#</a></h3><p><strong>基于 GNN 的架构优于其他模型和解决方案</strong>。上面列出的工作将他们的结果与浅层 ML、深度学习方法或特定任务的基线进行了比较。毫无疑问，<strong>GNN 的优越性是由于对拓扑和特征信息的考虑，这是以构建训练数据集的更大努力和更高的训练时间为代价的</strong></p><h4 id=ml对比>ML对比
<a class=anchor href=#ml%e5%af%b9%e6%af%94>#</a></h4><h5 id=shallow-ml><strong>shallow ML</strong>
<a class=anchor href=#shallow-ml>#</a></h5><ul><li>[42] 中，基于 GNN 的模型解决方案优于用于二元分类的经典 ML 技术，例如线性回归器 (LR)、支持向量机 (SVM)、随机森林和 MLP。与商业工业工具相比，它在不丢失故障覆盖率的情况下，插入的测试点减少了 11%，测试模式帐户减少了 6%。</li><li>GNN-RE [4] 中，将基于 GNN 的模型与使用每个节点相同特征向量的监督分类任务的 SVM 进行比较。但是，根据定义，SVM 只考虑单个节点的特征，不能访问邻居节点的特征。 SVM 的训练时间几乎是 GNN-RE [4] 的 10 倍。然而，分类指标清楚地表明了 GNN 的优越性。</li><li>ParaGraph [46] 与 XGBoost [6] 和 LR 模型以及普通 GraphSAGE 进行了比较。预测准确率平均为 77%，比 XGBoost 好 110%，比 GraphSAGE 好 7%。</li></ul><h5 id=deep-ml><strong>deep ML</strong>
<a class=anchor href=#deep-ml>#</a></h5><ul><li>D-SAGE [55] 与 MLP 和 vanilla GraphSAGE 进行了比较。**基于 GNN 的模型优于 MLP，强调结构信息与操作映射的相关性。**此外，<strong>由于边缘方向信息，D-SAGE [55] 相对于 GraphSAGE 获得了 17% 的相对增益。</strong></li><li>定制的 ABGNN [22] 在输入和输出边界分类任务方面都优于普通 GraphSAGE、GAT 和图同构网络 (GIN) [62]，同时推理时间分别减少了 19.8% 和 18.0%。与使用电平相关衰减和 (LDDS)-存在向量 (EV) 来表示电路拓扑 [11] 的 NN 相比，ABGNN [22] 的性能也更高且运行时间更低</li><li>[49] 中，Circuit Designer [59] 与非基于图的 RL 架构进行了比较。不同的实验表明了 GCN-RL 的优越性，它在非基于图的 RL 永远不会收敛的情况下收敛，即使有非常多的模拟步骤。 Circuit Designer [59] 的成功也归功于考虑线性晶体管参数的明确特征向量</li></ul><h4 id=compared-to-task-specific-baselines><strong>Compared to task-specific baselines.</strong>
<a class=anchor href=#compared-to-task-specific-baselines>#</a></h4><p>GNN 的优势与两个因素有关：<strong>定义明确的初始特征和图学习能力</strong> [39]。**定义明确的特征捕捉任务的基本特征，并为图学习提供宝贵的信息。 GNN 捕获特征和拓扑信息。**这与仅考虑图形连通性的方法相比，它具有明显的优势。</p><p>在 PL-GNN [39] 中，<strong>与层次相关的特征用于学习哪些节点相似，而与记忆相关的特征有助于平衡关键路径。</strong> PL-GNN [39] 优于基于模块化的聚类，后者仅使用连接信息</p><p>CongestionNet [29] 的先验拥塞估计性能优于基线方法 29%，而对于较低层的拥塞估计则优于 75%。尽管如此，CongestionNet [29] 训练需要在单个 GPU 上进行 60 小时。在推理期间，130 万个单元的运行时间减少到 19 秒，少于基线方法的运行时间。</p><p>Edge-GNN [44] 与开源全局布局工具 RePLAce3 [8] 和使用行业标准 EDA 工具的手动布局的结果进行了比较。在训练超过 10000 个芯片 loorplans 之后，Edge-GNN [44] 优于这两种方法，平均给出更低或相当的最差负松弛、总面积、功率和拥塞</p><h3 id=models-depth><strong>Models Depth</strong>
<a class=anchor href=#models-depth>#</a></h3><p>GNN 模型的层数是一个超参数。添加过多的层会使输出嵌入变得平滑并降低端到端任务的准确性。因此，<strong>更深层次的模型需要更大的数据集 [64]</strong>。！</p><h3 id=challenges><strong>Challenges</strong>
<a class=anchor href=#challenges>#</a></h3><h4 id=鲁棒性><strong>鲁棒性</strong>
<a class=anchor href=#%e9%b2%81%e6%a3%92%e6%80%a7>#</a></h4><h4 id=可解释性><strong>可解释性</strong>
<a class=anchor href=#%e5%8f%af%e8%a7%a3%e9%87%8a%e6%80%a7>#</a></h4><h4 id=预训练><strong>预训练</strong>
<a class=anchor href=#%e9%a2%84%e8%ae%ad%e7%bb%83>#</a></h4><h4 id=复杂图类型><strong>复杂图类型</strong>
<a class=anchor href=#%e5%a4%8d%e6%9d%82%e5%9b%be%e7%b1%bb%e5%9e%8b>#</a></h4><h2 id=gnns-in-eda-challenges><strong>GNNs in EDA Challenges</strong>
<a class=anchor href=#gnns-in-eda-challenges>#</a></h2><p>从 EDA 的角度来看，在将 GNN 应用于设计流程时，尤其是图形的可扩展性和多样性 [41] 时，先前的挑战会加剧。在 EDA 中，输入图代表不同抽象级别的电路网表，<strong>通常具有非常大的尺寸</strong>。大图会导致巨大的稀疏邻接矩阵和非常大的节点列表，其计算耗时且计算量大。！</p><p>与简单的无向图或有向图相比，EDA 对象在直觉上更类似于复杂的图类型。直观地，<strong>电路网表是有向超图或异构图</strong>，为了与这些图类型相结合，应该制定新的 GNN 架构，如 [22, 55] 中所做的那样。为 EDA 对象<strong>定制 GNN</strong> 架构并提高其处理复杂图形类型的性能仍然是一个开放的研究课题。</p><p><strong>数据生成</strong>仍然是一个开放的挑战</p><h2 id=future-work><strong>Future Work</strong>
<a class=anchor href=#future-work>#</a></h2><h3 id=exploiting-transfer-learning>Exploiting Transfer Learning
<a class=anchor href=#exploiting-transfer-learning>#</a></h3><p>​ CongestionNet [29] 等应用程序与技术无关，即它们根据所使用的技术构建具有特征向量和标签的数据集。<strong>对于针对新工艺技术的实验，应重新构建数据集并重新训练模型</strong>。我们期望未来的 EDA 应用程序评估 GNN 的<strong>迁移学习技术</strong>的使用，例如 [19、32、44、59] 中已经完成的。例如，GCN-RL Circuit Designer [59] 受益于 RL 的可转移性，并利用它在 5 个不同的技术节点和拓扑之间进行知识转移。</p><h3 id=exploiting-feature-information><strong>Exploiting Feature Information</strong>
<a class=anchor href=#exploiting-feature-information>#</a></h3><p><strong>GNN 的一个成功因素是定义明确的特征向量</strong>。因此，我们相信未来的研究将<strong>评估不同的特征节点，以提高当前解决方案对更先进技术和复杂任务的可扩展性和泛化性</strong>。</p><p>例如，<strong>当前的 Circuit Designer 实施仅使用线性晶体管参数作为特征。未来的工作应该涵盖非线性特征以支持非线性组件类型。</strong></p><p>在 [29] 中，进行了消融研究以确定特征对 CongestionNet 预测的重要性。因此，<strong>与cell类型或功能相关的特征比与cell几何形状相关的特征更重要</strong>。因此，<strong>未来的工作建议包括新的特征，如引脚和边缘类型</strong>。</p><p>此外，他们<strong>建议使用有向图或超图而不是无向图。</strong></p><h3 id=enlarging-datasets><strong>Enlarging Datasets</strong>
<a class=anchor href=#enlarging-datasets>#</a></h3><h2 id=开源工具>开源工具
<a class=anchor href=#%e5%bc%80%e6%ba%90%e5%b7%a5%e5%85%b7>#</a></h2><p>1https://github.com/The-OpenROAD-Project</p><p>2https://github.com/ALIGN-analoglayout/ALIGN-public</p><p>OpenROAD 和 ALIGN 都没有开始探索 GNN 的使用</p><h2 id=参考>参考
<a class=anchor href=#%e5%8f%82%e8%80%83>#</a></h2><p><a href=https://blog.csdn.net/sxf1061700625/article/details/127865492>【阅读】A Comprehensive Survey on Electronic Design Automation and Graph Neural Networks——EDA+GNN综述翻译_ppaml-CSDN博客</a></p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/alex-shpak/hugo-book/commit/867a96decfd822b12870ebe14a551d1f46c8f43f title='最后修改者 pxhg02 | 二月 20, 2025' target=_blank rel=noopener><img src=/svg/calendar.svg class=book-icon alt>
<span>二月 20, 2025</span></a></div><div><a class="flex align-center" href=https://github.com/alex-shpak/hugo-book/edit/main/exampleSite/content.zh/docs/Digtal/flow/notebak/EDA&amp;#43;GNN.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt>
<span>编辑本页</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#survey>Survey</a><ul><li><a href=#background--intro>Background & Intro</a><ul><li><a href=#np-complete>NP-complete</a></li><li><a href=#ml>ML</a></li></ul></li><li><a href=#eda>EDA</a><ul><li><a href=#flow>flow</a></li><li><a href=#挑战>挑战</a></li></ul></li><li><a href=#graphs><strong>Graphs</strong></a><ul><li><a href=#处理图结构数据的方法>处理图结构数据的方法</a></li></ul></li><li><a href=#gnn-1>GNN</a><ul><li><a href=#gnns-for-the-digital-eda-flow>GNNs for the Digital EDA Flow</a></li><li><a href=#类型>类型</a></li><li><a href=#gnn-的设计流程>GNN 的设计流程</a></li><li><a href=#gnn优点><strong>GNN优点</strong></a></li><li><a href=#models-depth><strong>Models Depth</strong></a></li><li><a href=#challenges><strong>Challenges</strong></a></li></ul></li><li><a href=#gnns-in-eda-challenges><strong>GNNs in EDA Challenges</strong></a></li><li><a href=#future-work><strong>Future Work</strong></a><ul><li><a href=#exploiting-transfer-learning>Exploiting Transfer Learning</a></li><li><a href=#exploiting-feature-information><strong>Exploiting Feature Information</strong></a></li><li><a href=#enlarging-datasets><strong>Enlarging Datasets</strong></a></li></ul></li><li><a href=#开源工具>开源工具</a></li><li><a href=#参考>参考</a></li></ul></li></ul></nav></div></aside></main></body></html>